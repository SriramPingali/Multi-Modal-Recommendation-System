{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9611126e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce5af4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e377cca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from numpy.ma import masked_where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90e5c3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5de9337",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b51f39a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(\"../Datasets/ml-100k/Text/items.csv\")\n",
    "\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "ratings = pd.read_csv('../Datasets/ml-100k/Text/u1.base', sep='\\t', names=r_cols,encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c54c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = 943\n",
    "n_items = 1682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8b3a425",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = np.zeros((n_users, n_items))\n",
    "for line in ratings.itertuples():\n",
    "    data_matrix[line[1]-1, line[2]-1] = line[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46e726f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix_emp = data_matrix.copy()\n",
    "data_matrix_emp[data_matrix < 4] = 0\n",
    "data_matrix_emp[data_matrix >= 4]= 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92c5e310",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = list(zip(*(np.where(data_matrix != 0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a79d09fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0ac7eb",
   "metadata": {},
   "source": [
    "# Siamese network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2daa12",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b274424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32bab2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ac66b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_csv = \"../Datasets/ml-100k/Text/items.csv\"\n",
    "train_ratings = \"../Datasets/ml-100k/Text/u1.base\"\n",
    "test_ratings = \"../Datasets/ml-100k/Text/u1.test\"\n",
    "\n",
    "item_path = \"../Datasets/ml-100k/\"\n",
    "ROW = 30\n",
    "BATCH = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54d80b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52f21b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovielensDataset(Dataset):\n",
    "    def __init__(self, ratings = train_ratings, item_path = item_path, device = device):\n",
    "        self.item_path = item_path\n",
    "#         self.video_embeddings = pd.read_csv(item_path + \"Video/embeddings.csv\").to_numpy()\n",
    "#         self.audio_embeddings = pd.read_csv(item_path + \"Audio/embeddings.csv\").to_numpy()\n",
    "        self.meta_embeddings = pd.read_csv(item_path + \"Meta/embeddings.csv\").to_numpy()\n",
    "        self.text_embeddings = pd.read_csv(item_path + \"Text/embeddings.csv\").to_numpy()\n",
    "        self.ratings = pd.read_csv(ratings, sep='\\t', \n",
    "                                   names=['user_id', 'movie_id', 'rating', 'unix_timestamp'],encoding='latin-1')\n",
    "        self.indices = None\n",
    "        self.device = device\n",
    "        self.data = None\n",
    "        self.n_users = None\n",
    "        self.n_items = None\n",
    "        self.fill_ratings()\n",
    "        self.embeddings()\n",
    "    \n",
    "    def fill_ratings(self, threshold=4):\n",
    "        self.n_users = self.ratings.user_id.unique().shape[0]\n",
    "        self.n_items = self.ratings.movie_id.unique().shape[0]\n",
    "        \n",
    "        self.data = np.zeros((n_users, n_items))\n",
    "        for line in self.ratings.itertuples():\n",
    "            self.data[line[1]-1, line[2]-1] = line[3]\n",
    "        \n",
    "        self.data_emp = np.where(np.logical_and(self.data > 3,\n",
    "                            np.random.random_sample(self.data.shape) <= 0.2), 1, 0)\n",
    "        self.indices = list(zip(*(np.where(self.data != 0))))\n",
    "    \n",
    "    def fill_med_embeddings(self, EXT = \"*.pkl\", type_ = \"audioEmbed/\"):\n",
    "        all_csv_files = [self.item_path + type_ + str(i) + \".pkl\" for i in range(1682)]\n",
    "\n",
    "        k = [normalize(np.load(file, allow_pickle = True)[:ROW], axis = 0) for file in tqdm(all_csv_files)]\n",
    "        return(np.stack(tuple(k)))\n",
    "        \n",
    "    def embeddings(self):\n",
    "        self.audio_embeddings = self.fill_med_embeddings(type_ = \"audioEmbed/\")\n",
    "        self.video_embeddings = self.fill_med_embeddings(type_ = \"videoEmbed/\")\n",
    "#         self.audio_embeddings = normalize(self.audio_embeddings, axis = 0)\n",
    "#         self.video_embeddings = normalize(self.video_embeddings, axis = 0)\n",
    "        self.user_embeddings = np.divide(np.dot(self.data_emp, self.meta_embeddings), \n",
    "                                         self.data_emp.sum(axis = 1)[:, None] + 0.001)\n",
    "#         self.user_embeddings = data_matrix\n",
    "        self.item_embeddings = data_matrix.T\n",
    "        self.video_embedding_size = self.video_embeddings.shape[2]\n",
    "        self.audio_embedding_size = self.audio_embeddings.shape[2]\n",
    "        self.text_embedding_size = self.text_embeddings.shape[1]\n",
    "        self.user_embedding_size = self.user_embeddings.shape[1]\n",
    "        self.item_embedding_size = self.item_embeddings.shape[1]\n",
    "        self.meta_embedding_size = self.meta_embeddings.shape[1]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(len(self.indices))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user = self.indices[idx][0]\n",
    "        item = self.indices[idx][1]\n",
    "        \n",
    "#         xu = self.user_embeddings(torch.LongTensor([user])).squeeze().to(self.device)\n",
    "        xu = torch.from_numpy(self.user_embeddings[user]).to(self.device)\n",
    "        xa = torch.from_numpy(self.audio_embeddings[item]).to(self.device)\n",
    "        xv = torch.from_numpy(self.video_embeddings[item]).to(self.device)\n",
    "        xt = torch.from_numpy(self.text_embeddings[item]).to(self.device)\n",
    "        xi = torch.from_numpy(self.item_embeddings[item]).to(self.device)\n",
    "        xm = torch.from_numpy(self.meta_embeddings[item]).to(self.device)\n",
    "        \n",
    "        y = self.data[user][item]\n",
    "        return(xu.float(), [xv.float(), xa.float(), xt.float(), xi.float(), xm.float()], int(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ee88fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1682/1682 [00:09<00:00, 179.58it/s]\n",
      "100%|██████████| 1682/1682 [00:00<00:00, 4891.98it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. the normalize function expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1423922b3f2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMovielensDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMovielensDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_ratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-3bb832130ce7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ratings, item_path, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_ratings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfill_ratings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-3bb832130ce7>\u001b[0m in \u001b[0;36membeddings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_med_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"videoEmbed/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#         self.user_embeddings = torch.nn.Embedding(943, 300)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         self.user_embeddings = np.divide(np.dot(self.data_emp, self.meta_embeddings), \n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[1;32m   1790\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'%d' is not a supported axis\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1793\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    792\u001b[0m                 ) from e\n\u001b[1;32m    793\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    795\u001b[0m                 \u001b[0;34m\"Found array with dim %d. %s expected <= 2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m                 \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. the normalize function expected <= 2."
     ]
    }
   ],
   "source": [
    "train_dataset = MovielensDataset(ratings = train_ratings)\n",
    "test_dataset = MovielensDataset(ratings = test_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(train_dataset, batch_size = BATCH, shuffle = True)\n",
    "testloader = DataLoader(test_dataset, batch_size = BATCH, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f943bcc",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17f8b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e938df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_metrics import mapk\n",
    "from recmetrics import mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824a5c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = np.array([np.count_nonzero(train_dataset.data == i) for i in range(1, 6)])\n",
    "weight = weight.max() / weight\n",
    "weight = torch.Tensor(weight).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd46f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse_loss(pred, target, weight=weight):\n",
    "    target = target.long()\n",
    "    weight = weight[target - 1].to(pred.dtype)\n",
    "    loss = (pred - target.to(pred.dtype)).pow(2)\n",
    "    return ((weight * loss).mean(), loss.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e99d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self, device = device, channel = 84):\n",
    "        super(SiameseNet, self).__init__()\n",
    "        self.encoder_user = nn.Sequential(OrderedDict([\n",
    "            ('linr1', nn.Linear(train_dataset.user_embedding_size, 1024)),\n",
    "            ('relu1', nn.LeakyReLU()),\n",
    "            ('linr2', nn.Linear(1024, channel)),\n",
    "            ('relu2', nn.LeakyReLU()),\n",
    "        ]))\n",
    "        \n",
    "        self.encoder_item = nn.Sequential(OrderedDict([\n",
    "            ('linr1', nn.Linear(train_dataset.item_embedding_size, 256)),\n",
    "            ('relu1', nn.LeakyReLU()),\n",
    "            ('linr2', nn.Linear(256, 300)),\n",
    "            ('relu2', nn.LeakyReLU()),\n",
    "        ]))\n",
    "        \n",
    "        self.encoder_video = nn.Sequential(OrderedDict([\n",
    "            ('linr1', nn.Linear(train_dataset.video_embedding_size, 1600)),\n",
    "            ('relu1', nn.LeakyReLU()),\n",
    "            ('linr2', nn.Linear(1600, 300)),\n",
    "            ('relu2', nn.LeakyReLU()),\n",
    "#             ('linr3', nn.Linear(1000, 500)),\n",
    "#             ('relu3', nn.LeakyReLU()),\n",
    "# #             ('norm2', nn.BatchNorm1d(500)),\n",
    "#             ('linr4', nn.Linear(500, 300)),\n",
    "#             ('relu4', nn.LeakyReLU()),\n",
    "        ]))\n",
    "        \n",
    "        self.encoder_audio = nn.Sequential(OrderedDict([\n",
    "            ('linr1', nn.Linear(train_dataset.audio_embedding_size, 600)),\n",
    "            ('relu1', nn.LeakyReLU()),\n",
    "            ('linr4', nn.Linear(600, 300)),\n",
    "            ('relu4', nn.LeakyReLU()),\n",
    "        ]))\n",
    "        \n",
    "        self.encoder_text = nn.Sequential(OrderedDict([\n",
    "            ('linr1', nn.Linear(train_dataset.text_embedding_size, 256)),\n",
    "            ('relu1', nn.LeakyReLU()),\n",
    "            ('linr2', nn.Linear(256, 300)),\n",
    "            ('relu2', nn.LeakyReLU()),\n",
    "        ]))\n",
    "        \n",
    "        self.encoder_meta = nn.Sequential(OrderedDict([\n",
    "            ('linr1', nn.Linear(train_dataset.meta_embedding_size, 1600)),\n",
    "            ('relu1', nn.LeakyReLU()),\n",
    "#             ('norm1', nn.BatchNorm1d(1600)),\n",
    "            ('linr2', nn.Linear(1600, 300)),\n",
    "            ('relu2', nn.LeakyReLU()),\n",
    "#             ('linr3', nn.Linear(1000, 500)),\n",
    "#             ('relu3', nn.LeakyReLU()),\n",
    "#             ('linr4', nn.Linear(500, 300)),\n",
    "#             ('relu4', nn.LeakyReLU()),\n",
    "        ]))\n",
    "        \n",
    "        self.fusion = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(1, 1, (20, 20), stride=(2, 2))),\n",
    "            ('relu1', nn.LeakyReLU()),\n",
    "            ('conv2', nn.Conv2d(1, 1, (5, 50), stride=(1, 1))),\n",
    "            ('relu2', nn.LeakyReLU()),\n",
    "        ]))\n",
    "        \n",
    "        self.siamese = nn.Sequential(OrderedDict([\n",
    "            ('linr1', nn.Linear(channel, 200)),\n",
    "            ('relu1', nn.LeakyReLU()),\n",
    "            ('linr2', nn.Linear(200, 256)),\n",
    "            ('relu2', nn.LeakyReLU()),\n",
    "            ('linr3', nn.Linear(256, 100)),\n",
    "            ('relu3', nn.LeakyReLU()),\n",
    "        ]))\n",
    "        \n",
    "        self.ffn = nn.Sequential(OrderedDict([\n",
    "            ('linr1', nn.Linear(300, 164)),\n",
    "            ('actv1', nn.ReLU()),\n",
    "            ('linr2', nn.Linear(164, 1)),\n",
    "#             ('actv2', nn.ReLU()),\n",
    "#             ('linr3', nn.Linear(50, 1)),\n",
    "        ]))\n",
    "        \n",
    "        self.device = device\n",
    "        self.encoder_user.apply(self.init_weights)\n",
    "        self.encoder_item.apply(self.init_weights)\n",
    "        self.encoder_video.apply(self.init_weights)\n",
    "        self.encoder_audio.apply(self.init_weights)\n",
    "        self.encoder_text.apply(self.init_weights)\n",
    "        self.encoder_meta.apply(self.init_weights)\n",
    "        self.siamese.apply(self.init_weights)\n",
    "        self.ffn.apply(self.init_weights)\n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "            \n",
    "    def exp(self, q, k, v):\n",
    "        z = torch.bmm(q, k.permute(0, 2, 1))\n",
    "        z = F.normalize(z, p = 10, dim = 1)\n",
    "        z = torch.softmax(z, 1)\n",
    "        z = torch.bmm(z, v)\n",
    "        return(z)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        # Modality-encoders\n",
    "        outu = self.encoder_user(x1)\n",
    "        outr = self.encoder_item(x2[3])\n",
    "        outv = torch.split(self.encoder_video(x2[0]), [100, 100, 100], 2)\n",
    "        outa = torch.split(self.encoder_audio(x2[1]), [100, 100, 100], 2)\n",
    "        outt = torch.split(self.encoder_text(x2[2]), 100, 1)\n",
    "        outm = torch.split(self.encoder_meta(x2[4]), [100, 100, 100], 1)\n",
    "        \n",
    "        # Attention\n",
    "        q_t = outt[0].unsqueeze(1).repeat(1, ROW, 1)\n",
    "        k_t = outt[1].unsqueeze(1).repeat(1, ROW, 1)\n",
    "        v_t = outt[2].unsqueeze(1).repeat(1, ROW, 1)\n",
    "#         print(q_t.shape, k_t.shape, v_t.shape)\n",
    "\n",
    "        q_a = outa[0]\n",
    "        k_a = outa[1]\n",
    "        v_a = outa[2]\n",
    "#         print(q_a.shape, k_a.shape, v_a.shape)\n",
    "        \n",
    "        q_v = outv[0]#.unsqueeze(1).repeat(1, ROW, 1)\n",
    "        k_v = outv[1]#.unsqueeze(1).repeat(1, ROW, 1)\n",
    "        v_v = outv[2]#.unsqueeze(1).repeat(1, ROW, 1)\n",
    "#         print(q_v.shape, k_v.shape, v_v.shape)\n",
    "        \n",
    "        q_m = outm[0].unsqueeze(1).repeat(1, ROW, 1)\n",
    "        k_m = outm[1].unsqueeze(1).repeat(1, ROW, 1)\n",
    "        v_m = outm[2].unsqueeze(1).repeat(1, ROW, 1)\n",
    "#         print(q_m.shape, k_m.shape, v_m.shape)\n",
    "        \n",
    "        # Self-Attention\n",
    "        st = self.exp(q_t, k_t, v_t)\n",
    "        sm = self.exp(q_m, k_m, v_m)\n",
    "        sa = self.exp(q_a, k_a, v_a)\n",
    "        sv = self.exp(q_v, k_v, v_v)\n",
    "\n",
    "        # Inter-Modal Attention\n",
    "        ita = self.exp(q_a, k_t, v_a)\n",
    "        imv = self.exp(q_v, k_m, v_v)\n",
    "        itm = self.exp(q_m, k_t, v_m)\n",
    "        \n",
    "        # Forward\n",
    "        ma = torch.mean(torch.stack([sa, ita]), 0)\n",
    "        mv = torch.mean(torch.stack([sv, imv]), 0)\n",
    "        sda = itm\n",
    "        se = torch.mul(ma, mv)\n",
    "        outi = torch.cat((sda, se), axis = 2)#.reshape(-1, ROW * 1200)\n",
    "        outi = self.fusion(outi.unsqueeze(1))\n",
    "        out1 = self.siamese(outu)\n",
    "        out2 = self.siamese(outi.reshape(BATCH, -1))\n",
    "        diff = torch.cat((out1, out2, outm[2]), axis=1)\n",
    "        out = self.ffn(diff)\n",
    "        return(out, out1, out2)\n",
    "    \n",
    "    def fit(self, trainloader = trainloader, \n",
    "            testloader = testloader, epochs = 100):\n",
    "        self.criterion_rate = weighted_mse_loss\n",
    "        self.criterion_embd = nn.CosineEmbeddingLoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = 1e-4)\n",
    "        \n",
    "        train_loss = []\n",
    "        train_f1 = []\n",
    "        test_loss = []\n",
    "        test_f1 = []\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            running_loss_1 = 0.0\n",
    "            \n",
    "            for i, data in tqdm(enumerate(trainloader)):\n",
    "                self.train()\n",
    "                x1, x2, y = data\n",
    "                y_flt = y.type(torch.FloatTensor).to(device)\n",
    "                y_lng = torch.div(y, 4, rounding_mode=\"floor\").to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                reg, outu, outi = self.forward(x1, x2)\n",
    "                loss_1, loss_ = self.criterion_rate(reg.squeeze(), y_flt)\n",
    "                loss_2 = self.criterion_embd(outu, outi, y_lng * 2 - 1)\n",
    "                loss = loss_1 + loss_2 \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                running_loss_1 += torch.sqrt(loss_)\n",
    "                running_loss += loss\n",
    "            vl, vp, vr, vf, tp, tr, tf = self.evaluate()\n",
    "            print('Epoch-%d: Loss = %.3f\\nTrain RMSE = %.3f||Train Precision = %.3f||Train Recall = %.3f\\nTest RMSE = %.3f || Test Precision = %.3f|| Test Recall = %.3f'%\n",
    "                  (epoch + 1, running_loss / i, running_loss_1 / i, \n",
    "                   tp, tr, vl, vp, vr))\n",
    "            train_loss.append((running_loss_1 / i).cpu().detach().numpy())\n",
    "            test_loss.append(vl.cpu().detach().numpy())\n",
    "            train_f1.append(tf)\n",
    "            test_f1.append(vf)\n",
    "        return(train_loss, test_loss, train_f1, test_f1)\n",
    "            \n",
    "    def evaluate(self, k = 3.5):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            valdata = next(iter(testloader))\n",
    "            x1, x2, y = valdata\n",
    "            y_flt = y.type(torch.FloatTensor).to(device)\n",
    "            y_lng = torch.div(y, 4, rounding_mode=\"floor\").to(device)\n",
    "            otpt = self.forward(x1, x2)\n",
    "            print(otpt[0])\n",
    "            pred = (otpt[0] > k).float()\n",
    "            vl = torch.sqrt(self.criterion_rate(otpt[0].squeeze(), y_flt)[1])\n",
    "            vp = precision_score(y_lng.cpu(), pred.cpu(), zero_division = 0)\n",
    "            vr = recall_score(y_lng.cpu(), pred.cpu(), zero_division = 0)\n",
    "            vf = f1_score(y_lng.cpu(), pred.cpu(), zero_division = 0)\n",
    "            print(classification_report(y_lng.cpu(), pred.cpu(),\n",
    "                    target_names = [\"0\", \"1\"], zero_division = 0))\n",
    "            \n",
    "            traindata = next(iter(trainloader))\n",
    "            x1, x2, y = traindata\n",
    "            y_flt = y.type(torch.FloatTensor).to(device)\n",
    "            y_lng = torch.div(y, 4, rounding_mode=\"floor\").to(device)\n",
    "            otpt = self.forward(x1, x2)\n",
    "            pred = (otpt[0] > k).float()\n",
    "            tp = precision_score(y_lng.cpu(), pred.cpu(), zero_division = 0)\n",
    "            tr = recall_score(y_lng.cpu(), pred.cpu(), zero_division = 0)\n",
    "            tf = f1_score(y_lng.cpu(), pred.cpu(), zero_division = 0)\n",
    "        return(vl, vp*100, vr*100, vf*100, tp*100, tr*100, tf*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356dfda8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm_net = SiameseNet()\n",
    "sm_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3fd8e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss, test_loss, train_f1, test_f1 = sm_net.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7fe6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sm_net, \"./pretrained/attention.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0516686",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_net.evaluate(k=3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d58c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax2 = ax.twinx()\n",
    "ax.plot(train_loss[20:], label = \"Train Loss\", color = \"orange\")\n",
    "ax2.plot(test_loss[20:], label = \"Test Loss\")\n",
    "fig.legend([ax, ax2], labels = [\"Train Loss\", \"Test Loss\"], loc = \"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b0befa",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac357e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845ce1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm_net = torch.load(\"./pretrained/rating.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd79f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prs = []\n",
    "# rec = []\n",
    "# f1 = []\n",
    "# loss = []\n",
    "\n",
    "# for i in np.arange(3, 4, 0.1):\n",
    "#     l, p, r, f, _, _, _ = sm_net.evaluate(i)\n",
    "#     prs.append(p)\n",
    "#     rec.append(r)\n",
    "#     f1.append(f)\n",
    "#     loss.append(l)\n",
    "    \n",
    "# plt.plot(prs, label = \"Test Precision\")\n",
    "# plt.plot(rec, label = \"Test Recall\")\n",
    "# plt.plot(f1, label = \"Test F1\")\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ce91bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8e0edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e5a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48029abc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
