{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e99f6d7a",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51ecc18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "436b3c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28efa8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7dad33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "687684d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(\"../Datasets/ml-100k/Text/items.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dba629d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = items['Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea9aac7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A little boy named Andy loves to be in his room, playing with his toys, especially his doll named \"Woody\". But, what do the toys do when Andy is not with them, they come to life. Woody believes that his life (as a toy) is good. However, he must worry about Andy\\'s family moving, and what Woody does not know is about Andy\\'s birthday party. Woody does not realize that Andy\\'s mother gave him an action figure known as Buzz Lightyear, who does not believe that he is a toy, and quickly becomes Andy\\'s new favorite toy. Woody, who is now consumed with jealousy, tries to get rid of Buzz. Then, both Woody and Buzz are now lost. They must find a way to get back to Andy before he moves without them, but they will have to pass through a ruthless toy killer, Sid Phillips. â€”John Wiggins'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3877effb",
   "metadata": {},
   "source": [
    "# AutoEncoder\n",
    "## Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50189246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package punkt to /home/sriram/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sriram/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sriram/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import wordnet\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, RepeatVector, Embedding, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "950933fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = wordnet.WordNetLemmatizer()\n",
    "sw = set(stopwords.words(\"english\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfee7a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder_Text(Model):\n",
    "    def __init__(self, input_=300, hidden=50, vocab=13672, train=True, data_text):\n",
    "        super(Autoencoder_Text, self).__init__()\n",
    "        self.max_len = input_\n",
    "        self.hidden = hidden\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.pad_sentence = self.pre_process(data_text, True)\n",
    "        emb_matrix, embedding_vector, embed_vector_len = self.read_glove_vector('./glove.6B.100d.txt')\n",
    "            \n",
    "        self.encoder = Sequential([\n",
    "            Embedding(input_dim = self.vocab, output_dim=embed_vector_len, \n",
    "                      input_length=self.max_len, weights=[emb_matrix], trainable=False),\n",
    "            Bidirectional(LSTM(self.hidden, return_sequences=False))\n",
    "        ])\n",
    "        \n",
    "        self.r_vec = RepeatVector(self.max_len)\n",
    "        self.decoder = Sequential([\n",
    "            Bidirectional(LSTM(self.hidden, return_sequences=True, dropout=0.2)),\n",
    "            TimeDistributed(Dense(self.vocab))\n",
    "        ])\n",
    "        self.enc_dec_model = Sequential([self.encoder, self.r_vec, self.decoder])\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r\"@[A-Za-z0-9]+\", ' ', text)\n",
    "        text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
    "        text = re.sub(r\"[^a-zA-z.!?'0-9]\", ' ', text)\n",
    "        text = re.sub('\\t', ' ',  text)\n",
    "        text = re.sub(r\" +\", ' ', text)\n",
    "        text = wt(text)\n",
    "        text = (\" \").join([lem.lemmatize(i, pos ='v') \n",
    "                           for i in text if i not in sw])\n",
    "        return(text)\n",
    "    \n",
    "    def tokenize(self, sentences):\n",
    "        text_tokenizer = Tokenizer()\n",
    "        text_tokenizer.fit_on_texts(sentences)\n",
    "        return(text_tokenizer.texts_to_sequences(sentences), text_tokenizer)\n",
    "    \n",
    "    def pre_process(self, data_text, train=False):\n",
    "        data_text = data_text.apply(self.clean_text)\n",
    "        text_tokenized, self.text_tokenizer = self.tokenize(data_text)\n",
    "        \n",
    "        if train:\n",
    "            self.vocab = len(self.text_tokenizer.word_index) + 1\n",
    "            self.max_len = int(len(max(text_tokenized,key=len)))\n",
    "            \n",
    "        pad_sentence = pad_sequences(text_tokenized, self.max_len, padding = \"post\")\n",
    "        return(pad_sentence.reshape(*pad_sentence.shape, 1))\n",
    "        \n",
    "    def read_glove_vector(self, glove_vec):\n",
    "        with open(glove_vec, 'r', encoding='UTF-8') as f:\n",
    "            words = set()\n",
    "            word_to_vec_map = {}\n",
    "            for line in f:\n",
    "                w_line = line.split()\n",
    "                curr_word = w_line[0]\n",
    "                word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)\n",
    "                \n",
    "        embed_vector_len = word_to_vec_map['moon'].shape[0]\n",
    "        emb_matrix = np.zeros((self.vocab, embed_vector_len))\n",
    "\n",
    "        for word, index in self.text_tokenizer.word_index.items():\n",
    "            embedding_vector = word_to_vec_map.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                emb_matrix[index, :] = embedding_vector\n",
    "        return(emb_matrix, embedding_vector, embed_vector_len)\n",
    "    \n",
    "#     def model(self):\n",
    "#         emb_matrix, embedding_vector, embed_vector_len = self.read_glove_vector('./glove.6B.100d.txt')\n",
    "#         self.encoder = Sequential([\n",
    "#             Embedding(input_dim = self.vocab, output_dim=embed_vector_len, \n",
    "#                       input_length=self.max_len, weights=[emb_matrix], trainable=False),\n",
    "#             Bidirectional(LSTM(self.hidden, return_sequences=False))\n",
    "#         ])\n",
    "        \n",
    "#         self.r_vec = RepeatVector(self.max_len)\n",
    "#         self.decoder = Sequential([\n",
    "#             Bidirectional(LSTM(self.hidden, return_sequences=True, dropout=0.2)),\n",
    "#             TimeDistributed(Dense(self.vocab))\n",
    "#         ])\n",
    "#         self.enc_dec_model = Sequential([self.encoder, self.r_vec, self.decoder])\n",
    "    \n",
    "    def train(self, data_text, epochs=10, batch_size=20):\n",
    "        # pad_sentence = self.pre_process(data_text, True)\n",
    "        # self.model()\n",
    "        self.__init__()\n",
    "        self.enc_dec_model.compile(loss=sparse_categorical_crossentropy,\n",
    "              optimizer=Adam(1e-3),\n",
    "              metrics=['accuracy'])\n",
    "        \n",
    "        self.enc_dec_model.summary()\n",
    "        self.enc_dec_model.fit(np.squeeze(self.pad_sentence, axis = 2), \n",
    "                                      self.pad_sentence, batch_size=batch_size, epochs=epochs)\n",
    "        self.enc_dec_model.save('./pretrained/text_model')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        pad_sentences = self.pre_process(inputs, False)\n",
    "        print(tf.convert_to_tensor(pad_sentences).shape)\n",
    "        return(self.enc_dec_model(tf.squeeze(pad_sentences, axis = 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249957c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 (13672, 100)\n",
      "100 (13672, 100)\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_10 (Sequential)   (None, 100)               1427600   \n",
      "_________________________________________________________________\n",
      "repeat_vector_4 (RepeatVecto (None, 240, 100)          0         \n",
      "_________________________________________________________________\n",
      "sequential_11 (Sequential)   (None, 240, 13672)        1441272   \n",
      "=================================================================\n",
      "Total params: 2,868,872\n",
      "Trainable params: 1,501,672\n",
      "Non-trainable params: 1,367,200\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1682/1682 [==============================] - 65s 39ms/step - loss: 3.8446 - accuracy: 0.7539\n",
      "Epoch 2/10\n",
      "1682/1682 [==============================] - 64s 38ms/step - loss: 3.0107 - accuracy: 0.7852\n",
      "Epoch 3/10\n",
      "1682/1682 [==============================] - 56s 33ms/step - loss: 3.0422 - accuracy: 0.7758\n",
      "Epoch 4/10\n",
      "1682/1682 [==============================] - 77s 46ms/step - loss: 2.9224 - accuracy: 0.7840\n",
      "Epoch 5/10\n",
      "1682/1682 [==============================] - 72s 43ms/step - loss: 2.8300 - accuracy: 0.7885\n",
      "Epoch 6/10\n",
      "1682/1682 [==============================] - 62s 37ms/step - loss: 2.8300 - accuracy: 0.7890\n",
      "Epoch 7/10\n",
      "1682/1682 [==============================] - 68s 40ms/step - loss: 2.7787 - accuracy: 0.7893\n",
      "Epoch 8/10\n",
      "1682/1682 [==============================] - 67s 40ms/step - loss: 2.8077 - accuracy: 0.7888\n",
      "Epoch 9/10\n",
      " 930/1682 [===============>..............] - ETA: 35s - loss: 2.9763 - accuracy: 0.7842"
     ]
    }
   ],
   "source": [
    "AE = Autoencoder_Text(data_text)\n",
    "AE.train(data_text, epochs = 10, batch_size = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b82e659",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.squeeze(AE.pre_process(data_text), axis = 2)\n",
    "output = AE.encoder.predict(inputs)\n",
    "print(output[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8964f31b-6b01-47aa-9686-f8c8ed7b0698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
